# -*- coding: utf-8 -*-
"""ListenerSpeaker_TF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MgJsX7Cai0EQRHsArU0L2MnGYt1JImYC
"""

# Commented out IPython magic to ensure Python compatibility.
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

try:
#   %tensorflow_version 2.x  # Colab only.
except Exception:
  pass

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
print(tf.__version__)

gpu_available = tf.test.is_gpu_available()
is_cuda_gpu_available = tf.test.is_gpu_available(cuda_only=True)
is_cuda_gpu_min_3 = tf.test.is_gpu_available(True, (3,0))

## Collab Pro
def monitor_gpu():
  gpu_info = !nvidia-smi
  gpu_info = '\n'.join(gpu_info)
  if gpu_info.find('failed') >= 0:
    print('Select the Runtime > "Change runtime type" menu to enable a GPU accelerator, ')
    print('and then re-execute this cell.')
  else:
    print(gpu_info)
monitor_gpu()

## Termination
!nvidia-smi
!sudo kill -9

## RAM
from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('To enable a high-RAM runtime, select the Runtime > "Change runtime type"')
  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')
  print('re-execute this cell.')
else:
  print('You are using a high-RAM runtime!')

print(tf.test.is_gpu_available())
print(tf.config.list_physical_devices('CPU'))
print(tf.config.list_physical_devices('GPU'))
print(tf.config.list_physical_devices('TPU'))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sys, os

# Kaggle DataSet
# ! pip install -q kaggle
!pip install --upgrade --force-reinstall --no-deps kaggle # force install of latest version
from google.colab import files
files.upload() # uncomment if no kaggle.json present in colab
# Make directory named kaggle and copy kaggle.json file there.
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
# Change Permisson
! chmod 600 ~/.kaggle/kaggle.json
# # Check if everying is working
# ! kaggle datasets list
# # Downloading competetion data
# !kaggle competitions download -c gan-getting-started
# !ls
# from kaggle_datasets import KaggleDatasets

# ! kaggle datasets list
# Downloading data from Kaggle competition
! kaggle competitions download -c 11785-hw4-fall2018

!unzip 11785-hw4-fall2018.zip -d hw4_data

"""## Data Loading"""

LETTER_LIST = ['<pad>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', \
               'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '-', "'", '.', '_', '+', ' ','<sos>','<eos>']


'''
Optional, create dictionaries for letter2index and index2letter transformations
    '<sos>': 33
    '<eos>': 34
'''
def create_dictionaries(letter_list):
    n = len(letter_list)
    letter2index = {letter_list[i]:i for i in range(0, n)}
    index2letter = {i:letter_list[i] for i in range(0, n)}
    return letter2index, index2letter

letter2index, index2letter = create_dictionaries(LETTER_LIST)

'''
Loading all the numpy files containing the utterance information and text information
- Input: training utterances each of variable duration and 40 frequency bands (L, F=40)
- Target: the transcripts corresponding to the utterances in train/dev; Shape: (B, )
'''
def load_data(dataPath):
    speech_train = np.load(dataPath+"train.npy", allow_pickle=True, encoding='bytes')
    speech_dev = np.load(dataPath+"dev.npy", allow_pickle=True, encoding='bytes')
    speech_test = np.load(dataPath+"test.npy", allow_pickle=True, encoding='bytes')

    transcript_train = np.load(dataPath+"train_transcripts.npy", allow_pickle=True, encoding='bytes')
    transcript_dev = np.load(dataPath+"dev_transcripts.npy", allow_pickle=True, encoding='bytes')

    return speech_train, speech_dev, speech_test, transcript_train, transcript_dev

'''
Transforms alphabetical input to numerical input, replace each letter by its corresponding
index from letter_list
  '<sos>': 33
  '<eos>': 34
'''
def transform_letter_to_index(transcript):
    '''
    :param transcript :(N, ) Transcripts are the text input
    :- letter2index: Letter list defined above, map letter to index from LETTER_LIST
    :return letter_to_index_list: Returns a list for all the transcript sentence to index
    '''
    letter_to_index_list = []
    for sent in transcript:
        letters = [letter2index['<sos>']] # start of sequence
        for word in sent:
            # Converte from byte format to string for mapping
            s = word.decode('utf-8')
            for c in s:
                letters.append(letter2index[c])
            # Space between each word
            letters.append(letter2index[' '])
        # pop the ending space, and replace with '<eos>'
        letters.pop()
        letters.append(letter2index['<eos>'])
        letter_to_index_list.append(letters)
    return letter_to_index_list


def transform_index_to_letter(index, stopIdxs, igonre_idxs=[]):
    index_to_letter_list = []
    for r in index:
        curr = ""
        for i in r:
            # Reached the end of the sentence
            if i in stopIdxs:
                break
            elif i in igonre_idxs:
                continue
            else:
                curr += index2letter[i]
        index_to_letter_list.append(curr)
    return index_to_letter_list

data_path = 'hw4_data/'

speech_train, speech_dev, speech_test, transcript_train, transcript_dev = load_data(data_path)

## Debug
print(type(speech_train))
print(type(speech_train[0]))
print(speech_train.shape)
print(speech_train[0].shape)
print(speech_train[0])
print(transcript_train.shape)
print(transcript_train[0])

for word in transcript_train[0]:
  # convert from byte format to string
  print('Byte Word: {}, Converted Word: {}'.format(word, word.decode('utf-8')))

# Preprocess transcript to char level index
character_text_train = transform_letter_to_index(transcript_train)
character_text_dev = transform_letter_to_index(transcript_dev)

## Debug
print(transcript_train.shape)
print(transcript_train[0])
print(len(character_text_train))
print(character_text_train[0])

print("Raw script: \n", transcript_dev[0])
print("Indexed script: \n", character_text_dev[0])
print("Recovered script: \n", transform_index_to_letter(character_text_dev, [])[0])
print("Recovered script with no special character(s): \n", transform_index_to_letter(character_text_dev, [letter2index['<eos>'], letter2index['<pad>']], igonre_idxs=[letter2index['<sos>']])[0])

## Load Data into DataSets, Default method
# speech_dev.astype(np.float32)
# speech_dev.astype(np.float32)
# train_ds = tf.data.Dataset.from_tensor_slices(speech_dev)

## DEV
#
dataset = tf.data.Dataset.range(100)

# Batching tensors with padding
dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))

# list(dataset.as_numpy_iterator())

## Load Dataset from Generator
def gen_dev():
  for utter, trans in zip(speech_dev, character_text_dev):
    # print(utter.shape) # comment during use...
    utter.astype(np.float32)
    
    utter_len = len(utter)
    trans_len = len(trans)
    # print(utter, trans, utter_len, trans_len)
    yield utter, trans, utter_len, trans_len

def gen_train():
  for utter, trans in zip(speech_train, character_text_train):
    # print(utter.shape) # comment during use...
    utter.astype(np.float32)
    
    utter_len = len(utter)
    trans_len = len(trans)
    # print(utter, trans, utter_len, trans_len)
    yield utter, trans, utter_len, trans_len

# gen(speech_dev, character_text_dev)
# gen()

# Create dataset from generator
# The output shape is variable: (None,)
# dataset = tf.data.Dataset.from_generator(gen, tf.int64, tf.TensorShape([None]))

# '''args have to be Tensor...'''
ds = tf.data.Dataset.from_generator(gen_dev, output_types=(tf.float32, tf.int32, tf.int32, tf.int32), output_shapes=(tf.TensorShape([None, 40]), tf.TensorShape([None]), (), ())) #tf.data.Dataset.from_generator(gen, args=[], output_types=tf.float32, output_shapes = (), )

ds_train = tf.data.Dataset.from_generator(gen_train, output_types=(tf.float32, tf.int32, tf.int32, tf.int32), output_shapes=(tf.TensorShape([None, 40]), tf.TensorShape([None]), (), ()))

print(len(list(ds.as_numpy_iterator())))
for batch in ds.take(2):
  print(type(batch))
  print(batch[0].shape)
  print(batch[0].numpy())
  print(batch[1].shape)
  print(batch[1].numpy())
  print(batch[2])
  print(batch[3])
  print("=====================")

print(len(list(ds_train.as_numpy_iterator())))
for batch in ds_train.take(2):
  print(type(batch))
  print(batch[0].shape)
  print(batch[0].numpy())
  print(batch[1].shape)
  print(batch[1].numpy())
  print(batch[2])
  print(batch[3])
  print("=====================")

"""### PreProcessing"""

## Pre-Processing

### shuffle
ds_shuffle = ds.shuffle(1024)

ds_train_shuffle = ds_train.shuffle(25000)

print(len(list(ds_shuffle.as_numpy_iterator())))
for batch in ds_shuffle.take(2):
  print(type(batch))
  print(batch[0].shape)
  print(batch[0].numpy())
  print(batch[1].shape)
  print(batch[1].numpy())
  print(batch[2])
  print(batch[3])
  print("=====================")

# lib padded batching
# Batching tensors with padding
batched_ds = ds.padded_batch(32)

batched_ds_train = ds_train_shuffle.padded_batch(16) # OOM DEBUG

print(len(list(batched_ds.as_numpy_iterator())))
### Generator Padded inputs
for batch in batched_ds.take(1):
  print(type(batch))
  print(type(batch[0]))
  print(batch[0].shape)
  print(batch[0][0].numpy()) # first utterance (T, C/F=40)
  print(batch[2].shape)
  print(batch[2].numpy())


for batch in batched_ds_train.take(1):
  print(type(batch))
  print(type(batch[0]))
  print(batch[0].shape)
  print(batch[0][0].numpy()) # first utterance (T, C/F=40)
  print(batch[2].shape)
  print(batch[2].numpy())

### Generator Padded targets
for batch in batched_ds.take(1):
  print(batch[3].numpy())
  print(batch[1].numpy())

## map/collate func

def collate_fn(input, target, input_len, target_len):
  # print(input.eval(session=tf.compat.v1.Session()))
  inputs_pad = []
  inputs_len = [] # same as input_len
  targets_pad = []
  targets_len = []
  
  for i in range(len(input)):
    utter_len = input_len[i]
    real_utter = input[i][:utter_len]
    inputs_pad.append(real_utter)

    trans_len = target_len[i]
    real_trans = target[i][1:trans_len] # ignore '<sos>'
    targets_pad.append(real_trans)
    targets_len.append(len(real_trans)) # tarns_len - 1
  
  # (B, T, F/C)
  padded_seqs = tf.keras.preprocessing.sequence.pad_sequences(inputs_pad, padding='post', dtype='float32')
  padded_targets = tf.keras.preprocessing.sequence.pad_sequences(targets_pad, padding='post', dtype='int32')
  # print(type(padded_seqs), type(padded_targets), type(input_len), type(targets_len))
  return tf.convert_to_tensor(padded_seqs), tf.convert_to_tensor(padded_targets), input_len, tf.convert_to_tensor(targets_len)

result = batched_ds.map(lambda input, target, input_len, target_len : tf.py_function(func=collate_fn, inp=[input, target, input_len, target_len], Tout=[tf.float32, tf.int32, tf.int32, tf.int32]))   # batched_ds.map(collate_fn)

result_train = batched_ds_train.map(lambda input, target, input_len, target_len : tf.py_function(func=collate_fn, inp=[input, target, input_len, target_len], Tout=[tf.float32, tf.int32, tf.int32, tf.int32]), num_parallel_calls=tf.data.AUTOTUNE,
    deterministic=False)
print(len(list(result.as_numpy_iterator())))

print(len(list(result_train.as_numpy_iterator())))

## Comparing targets with generator padded version
for batch in result.take(1):
  # length --, removed '<sos>'
  print(type(batch[1]), type(batch[3]))
  print(batch[3].numpy())
  print(batch[1].numpy())

## Comparing inputs/utterances with generator padded version
for batch in result.take(1):
  print(batch[0].shape)
  print(batch[2].numpy())

## DEBUG
### Compare first utterance
for d, b1, b2 in zip(speech_dev[:32], batched_ds.take(1), result.take(1)): #First batch
  print(b1[0].shape)
  print(b2[0].shape)
  print(b1[0][0].numpy())
  print(b2[0][0].numpy())
  diff = b1[0][0].numpy() == b2[0][0].numpy()
  print(diff)
  print(len(np.where(diff == False)[0]))

  # original
  print(d)
  utter = b2[0][0].numpy()
  # print(utter[:d.shape[0]])
  print(utter[:b2[2].numpy()[1]])
  diff_ = d == utter[:b2[2].numpy()[0]] # get utterence_len for first utterence
  print(diff_)
  print(len(np.where(diff_ == False)[0]))

## pad_sequences util
# print(speech_dev[:3])

sequences = [[1], [2, 3], [4, 5, 6]]

padded_seqs = tf.keras.preprocessing.sequence.pad_sequences(speech_dev[:3], padding='post', dtype='float32')

print(padded_seqs[0].shape)
print(speech_dev[0].shape)
print(speech_dev[0][0])
print(padded_seqs[0][0])
print(padded_seqs[0][:440].shape)

diff = padded_seqs[0][:440] == speech_dev[0]
print(len(np.where(diff == False)[0]))

### DEV
a = np.array([[3,2,1], [3,7,9]])
len(a)

def gen_():
    for i in range(1, 5):
        a =np.asarray(([i] * i)*i).reshape(i, i)
        
        # print(a)
        yield a

# Create dataset from generator
# The output shape is variable: (None,)
dataset = tf.data.Dataset.from_generator(gen_, tf.int64, tf.TensorShape([None, None]))

list(dataset.as_numpy_iterator())

list(dataset.take(3).as_numpy_iterator())

# gen_()
tf.convert_to_tensor([0,1,2], dtype=tf.int32)

# tf.convert_to_tensor(speech_dev, dtype=tf.float32)



"""### Model Structure"""

### Dev

#### BroadCast
# B = 3
l =  tf.constant([3, 2, 5])

print(type(l), l.shape)

l = tf.expand_dims(l,axis=1)
print(type(l), l.shape)

print(l.numpy())

r = tf.range(5)
r = tf.expand_dims(r, axis=0)
print(type(r), r.shape)
print(r.numpy())

# (1, L_max), (B, 1)
mask = r >= l
print(mask.shape) # broadcast
print(mask.numpy())



"""### Encoder"""

import tensorflow.keras.layers as L
from tensorflow.keras import Model
import time

def encoder_fn(lstm_dim, input_dim=40):
    OUTPUT_CHANNELS = 3
    # B x L(max_len of current batch)x C
    inputs = L.Input(shape=(None, input_dim), name='input utterances')

    ### 
    # return the final state
    x, final_memory_state, final_carry_state = L.LSTM(lstm_dim, return_sequences=True, return_state=True, name='enc_dummy_lstm')(inputs) # B x L x H

    print("output Shape ", x.shape)
    print("Memory State Shape ", final_memory_state.shape)
    print("Carry State Shape ", final_carry_state.shape)
    ### Bi-LSTM
    # lstm_layer = L.LSTM(lstm_dim, return_sequences=True, return_state=False) # return the final state (tuple)
    # x = L.Bidirectional(lstm_layer)(inputs) # B x L x 2*H

    

    model = Model(inputs, [x, final_memory_state, final_carry_state])
    
    return model #, (final_memory_state, final_carry_state)

encoder = encoder_fn(lstm_dim=128)
## Concatenation of 
## Weight Matrixes
# (I x H | I x H | I x H | I x H)
# (H x H | H x H | H x H | H x H)
encoder.summary()

## Debug
train_vars = encoder.trainable_variables
print(type(train_vars)) # sequence of trainable variables
print(len(train_vars)) 
print(type(train_vars[0]))
for var in train_vars:
  print(var.shape)
  print(var)

train_ws = encoder.trainable_weights
print(type(train_ws))
print(len(train_ws))
for w in train_ws:
  print(w.shape)

ws = encoder.get_weights()
print(type(ws))
print(len(ws))

for w in ws:
  print(w.shape)
  print(w)


def inspect_wieghts(model):
  ws = model.get_weights()
  train_vars = model.trainable_variables
  print(type(ws))
  print(len(ws))

  for w, var in zip(ws, train_vars):
    print("Variable: "+var.name)
    print(w.shape)
    print(w)

"""### Decoder"""

'''
  @param vocab_size: dictionary size
  @states each state should [B, dec_hidden]
'''
def decoder_fn(vocab_size, embed_dim, dec_hidden, states=[None, None]):
  
  # for single timestamp (B, T)
  inputs = L.Input(shape=(None,), name='input target')

  ## Embedding 
  embed = L.Embedding(vocab_size, embed_dim)(inputs) # (B, T, E)

  print(embed.shape)

  ## LSTM
  # lstm = L.LSTMCell(dec_hidden)(embed) # (B, H)

  x, final_memory_state, final_carry_state = L.RNN(L.LSTMCell(dec_hidden, name='dummy_lstm_cell'), return_sequences=True, return_state=True, name='dec_dummy_rnn')(embed)

  print(x.shape)

  # return x
  #Final Dense layer on which softmax will be applied
  # "linear" activation: a(x) = x).
  fc = L.Dense(vocab_size)(x)

  return Model(inputs, fc) #Model(inputs, [x, final_memory_state, final_carry_state])



# if we feed encoder final state to decoder, have to share same hidden size
decoder = decoder_fn(len(LETTER_LIST), 100, 128)
decoder.summary()

inspect_wieghts(decoder)

# Debug
example_batch = next(iter(result))
print(example_batch[0].shape)
print(example_batch[1].shape) # (B, max taraget length)

example_output, example_h, example_c = encoder(example_batch[0])
print(type(example_output))
print ('Encoder output shape: (batch size, sequence length, units/enc_hidden) {}'.format(example_output.shape))
print ('Encoder h vecotr shape: (batch size, units) {}'.format(example_h.shape))
print ('Encoder c vector shape: (batch size, units) {}'.format(example_c.shape))

print('hidden state of last timestep')
print(example_h.numpy())
print('Last Feature output of LSTM')
print(example_output.numpy()[:, -1, :])

example_dec_output = decoder(example_batch[1]) # input targets

print ('Decoder output shape: (batch size, target length, units/dec_hidden) {}'.format(example_dec_output.shape))

example_ts_target = example_batch[1][:, 0]
print(example_batch[1].numpy())
print(example_ts_target.numpy())

"""#### LSTMCell based decoder"""

def decoder_block(x, vocab_size, embed_dim, dec_hidden, states=[None, None]):
  
  # for single timestamp (B, 1)

  ## Embedding 
  embed = L.Embedding(vocab_size, embed_dim)(x) # (B, 1, E)

  print(embed.shape)

  ## LSTM
  lstm = L.LSTMCell(dec_hidden)(embed, states) # (B, H)

  return lstm

dec_block_output, dec_block_state = decoder_block(example_ts_target, len(LETTER_LIST), 100, 128, [example_h, example_c])

print(type(dec_block_state))
print(dec_block_state[0].shape)
print(dec_block_state[1].shape) # Sharing the HiddenSize between enc and dec
print(dec_block_output.shape) # decoder output for single timestamp (BatchSize, DecHiddenSize)
print(dec_block_output.numpy())
print(dec_block_state[0].numpy())
print(dec_block_state[1].numpy())



"""#### Module Decoder"""

'''
  @param vocab_size: dictionary size
  @states each state should [B, dec_hidden]
'''


class Decoder(tf.keras.Model):
  def __init__(self, vocab_size, embed_dim, dec_hidden):
    super(Decoder, self).__init__()
    
    self.dec_units = dec_hidden
    self.embedding = L.Embedding(vocab_size, embed_dim)

    ##________ LSTM layer in Decoder ------- ##
    self.lstm_layer = L.RNN(L.LSTMCell(dec_hidden, name='dummy_lstm_cell'), return_sequences=True, return_state=True, name='dec_dummy_rnn')

    self.fc = L.Dense(vocab_size)



  def call(self, x, hidden):
    x = self.embedding(x)
    output, h, c = self.lstm_layer(x, initial_state = hidden)
    dis = self.fc(output)
    return dis, h, c

# take in variable length input
decoder_ = Decoder(len(LETTER_LIST), 100, 128)
# decoder_.build(tf.TensorShape(None))
# inspect_wieghts(decoder_)


example_dec_output, ex_dec_h, ex_dec_c = decoder_(example_batch[1], None)
decoder_.summary()
inspect_wieghts(decoder_)

print(example_dec_output.shape) # （B, T_max, V)
print ('Decoder h vecotr shape: (batch size, dec_units) {}'.format(ex_dec_h.shape))
print ('Decoder c vector shape: (batch size, dec_units) {}'.format(ex_dec_c.shape))

"""## Train"""

# DEV
y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
# Using 'auto'/'sum_over_batch_size' reduction type.
cce = tf.keras.losses.CategoricalCrossentropy()
cce(y_true, y_pred).numpy()

# Reduction by sum
cce = tf.keras.losses.CategoricalCrossentropy(
    reduction=tf.keras.losses.Reduction.SUM)
cce(y_true, y_pred).numpy()

def loss_function(real, pred):
  # loss always reduce -1 dimension
  # real shape = (BATCH_SIZE, max_length_output)
  # pred shape = (BATCH_SIZE, max_length_output, target_vocab_size )
  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')
  loss = cross_entropy(y_true=real, y_pred=pred)
  ## TODO: Mased Loss
  # mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1
  # mask = tf.cast(mask, dtype=loss.dtype)  
  # loss = mask* loss
  loss = tf.reduce_mean(loss)
  return loss

optimizer = tf.keras.optimizers.Adam()

@tf.function( experimental_relax_shapes=True)
def train_step(input, target):
  # print("=========== Inspecting weights of encoder")

  # inspect_wieghts(encoder)

  # print("=========== Inspecting weights of decoder")

  # inspect_wieghts(decoder_)

  loss = 0

  # print(input.shape)
  # print(target.shape)
  # print(target.numpy())

  # attach previous removed '<sos>'
  # Ignore '<eos>'
  # dec_input = tf.pad(target[:, :-1], [[0, 0], [1, 0]], "CONSTANT", constant_values=letter2index['<sos>'])
  # print(dec_input)
  with tf.GradientTape() as tape:
    enc_output, enc_h, enc_c =  encoder(input)

    # attach previous removed '<sos>'
    # Ignore '<eos>'
    dec_input = tf.pad(target[:, :-1], [[0, 0], [1, 0]], "CONSTANT", constant_values=letter2index['<sos>'])
    # real = target[ : , 1: ]         # ignore <start> token
    # target already have '<sos>' removed

    ## $$ use decoder final state as initial state for decoder LSTM cell here
    ## s and h sharing same size, so s-1 = hN
    ### Connecting Encoder and Decoder here to avoid `WARNING:tensorflow:Gradients do not exist for Encoder Variables when minimizing the loss.`

    pred, dec_h, dec_c = decoder_(dec_input, [enc_h, enc_c]) # input targets
    
    loss = loss_function(target, pred)

  variables = encoder.trainable_variables + decoder_.trainable_variables

  # # Calculate the gradients for encoder and decoder
  gradients = tape.gradient(loss, variables)
  optimizer.apply_gradients(zip(gradients, variables))

  return loss

EPOCHS = 10
data = result_train # dev dataset: result
for epoch in range(EPOCHS):
  start = time.time()
  total_loss = 0
  # print(enc_hidden[0].shape, enc_hidden[1].shape)
  steps_per_epoch = 0

  for (batch, (inp, targ, input_lens, target_lens)) in enumerate(data):
    # Debug
    # print(type(inp), type(targ), type(input_lens), type(target_lens))
    batch_loss = train_step(inp, targ)
    total_loss += batch_loss

    if batch % 10 == 0:
      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,
                                                   batch,
                                                   batch_loss))
                                                  #  batch_loss.numpy())) # tf function

    steps_per_epoch += 1
  # # saving (checkpoint) the model every 2 epochs
  # if (epoch + 1) % 2 == 0:
  #   checkpoint.save(file_prefix = checkpoint_prefix)

  print('Epoch {} Loss {:.4f}, after {} steps'.format(epoch + 1,
                                      total_loss / steps_per_epoch, steps_per_epoch))
  print('Time taken for 1 epoch {} sec\n'.format(time.time() - start)) # CPU: 86s for dev_data

  # Resource Monitoring
  monitor_gpu()

## Debug
# for batch in result_train:
#   print(batch[0].shape, batch[1].shape, batch[3].shape)

monitor_gpu()

# debug

inspect_wieghts(decoder_)

inspect_wieghts(encoder)

## DEV
t = tf.constant([[1,2,3], [2,3,4], [5,6,6]])
# t = t + tf.fill(3, 33)
f = tf.fill(3, 1)
print(f)

# tf.stack([t, f], axis=0)
# tf.concat([t, f], axis=1)
tf.pad(t, [[0, 0], [1, 0]], "CONSTANT", constant_values=33)

r = tf.random.uniform(shape=[2, 3, 2])
r.numpy()

r_ = r[:, 0, :] # take the first sequence
r_.numpy()



"""## Inference/Test"""

print(example_dec_output.shape) # （B, T_max, V)
print(example_dec_output)

test_batch = next(iter(result))
print(test_batch[0].shape)
print(test_batch[1].shape) # (B, max taraget length)

# it = iter(result)
# for b in result.take(10):
#   print(b[0].shape)

def val(num_samples=2, max_seq=20):
  # For single Batch
  utter_input = test_batch[0] # B, U_max, F
  target_trans = test_batch[1] # B, T_max
  batch_size = target_trans.shape[0]
  print(utter_input.shape)
  print(target_trans.shape)

  og_trans = transform_index_to_letter(target_trans.numpy(), [letter2index['<eos>'], letter2index['<pad>']])
  for i in range(target_trans[:min(num_samples, batch_size)].shape[0]):
    utter = utter_input[i:i+1, :, :] # avoid squeezing
    print(utter.numpy())
    target = target_trans[i:i+1, :]
    print(utter.shape) # (1, U, F)
    print(target.shape) # (1, T)
    
    # Invoke Encoder
    enc_output, enc_h, enc_c =  encoder(utter)
    print(enc_output.shape) # 1, U, enc_dim
    print(enc_h.shape) # 1, enc_dim

    # attach previous removed '<sos>'
    # Ignore '<eos>', $$$ TODO: need to remove padding
    # dec_input = tf.pad(target[:, :-1], [[0, 0], [1, 0]], "CONSTANT", constant_values=letter2index['<sos>'])
    # print(dec_input)
    dec_input = tf.fill([1, 1], letter2index['<sos>']) # start of sequence # (1, 1)
    seq = ['<sos>']
    seq_index = [letter2index['<sos>']]
    # initial Decoder State
    # Use Encoder hidden states for now, s-1=hN
    h_state = enc_h
    c_state = enc_c

    while tf.squeeze(dec_input).numpy() != letter2index['<eos>'] and len(seq) < max_seq:
      # print(h_state.numpy())
      # print(c_state.numpy())
      pred, dec_h, dec_c = decoder_(dec_input, [h_state, c_state]) # input targets 
      h_state = dec_h
      c_state = dec_c
      # print("Last Linear Layer :", pred.shape) # (1, 1, 35)
      # print("Hidden state of last timestamp/current :", dec_h.shape) # 1, dec_dim
      # print("Hidden Cell state of last timestamp/current: ", dec_c.shape) # 1, dec_dim
      # print(pred[-1]) # (1, 1, 35)


      # $$ Greedy Search
      next_input =  tf.math.argmax(tf.squeeze(pred), axis=-1)
      dec_input = tf.fill([1,1], next_input)
      seq.append(index2letter[next_input.numpy()])
      seq_index.append(next_input.numpy())
      # print(seq)



    # print(utter.numpy())
    print("Input Target: ", target.numpy())
    print("Actual Transcript: ", og_trans[i])

    print("Raw Prediction sequence :", seq)
    print("Predicted Transcript: ", transform_index_to_letter([seq_index], [letter2index['<eos>'], letter2index['<pad>']]))

val(num_samples=10)

# after training on dev dataset
val(num_samples=10)

# after training on train dataset, 10 epoches with 100 steps per epoch, 32 batch_size
val(num_samples=10)

