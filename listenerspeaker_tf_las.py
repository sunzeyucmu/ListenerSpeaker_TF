# -*- coding: utf-8 -*-
"""ListenerSpeaker_TF_LAS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NtvihQ8HpTvSSwd7mRrghPe4bATSWprQ
"""

# Commented out IPython magic to ensure Python compatibility.
# Install TensorFlow
# !pip install -q tensorflow-gpu==2.0.0-beta1

try:
#   %tensorflow_version 2.x  # Colab only.
except Exception:
  pass

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
print(tf.__version__)

gpu_available = tf.test.is_gpu_available()
is_cuda_gpu_available = tf.test.is_gpu_available(cuda_only=True)
is_cuda_gpu_min_3 = tf.test.is_gpu_available(True, (3,0))

## Collab Pro
def monitor_gpu():
  gpu_info = !nvidia-smi
  gpu_info = '\n'.join(gpu_info)
  if gpu_info.find('failed') >= 0:
    print('Select the Runtime > "Change runtime type" menu to enable a GPU accelerator, ')
    print('and then re-execute this cell.')
  else:
    print(gpu_info)
monitor_gpu()

## RAM
from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('To enable a high-RAM runtime, select the Runtime > "Change runtime type"')
  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')
  print('re-execute this cell.')
else:
  print('You are using a high-RAM runtime!')

print(tf.test.is_gpu_available())
print(tf.config.list_physical_devices('CPU'))
print(tf.config.list_physical_devices('GPU'))
print(tf.config.list_physical_devices('TPU'))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sys, os, time
import tensorflow.keras.layers as L
from tensorflow.keras import Model

# Kaggle DataSet
# ! pip install -q kaggle
!pip install --upgrade --force-reinstall --no-deps kaggle # force install of latest version
from google.colab import files
files.upload() # uncomment if no kaggle.json present in colab
# Make directory named kaggle and copy kaggle.json file there.
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
# Change Permisson
! chmod 600 ~/.kaggle/kaggle.json
# # Check if everying is working
# ! kaggle datasets list
# # Downloading competetion data
# !kaggle competitions download -c gan-getting-started
# !ls
# from kaggle_datasets import KaggleDatasets

# ! kaggle datasets list
# Downloading data from Kaggle competition
! kaggle competitions download -c 11785-hw4-fall2018

!unzip 11785-hw4-fall2018.zip -d hw4_data

"""## Data Loading"""

LETTER_LIST = ['<pad>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', \
               'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '-', "'", '.', '_', '+', ' ','<sos>','<eos>']


'''
Optional, create dictionaries for letter2index and index2letter transformations
    '<sos>': 33
    '<eos>': 34
'''
def create_dictionaries(letter_list):
    n = len(letter_list)
    letter2index = {letter_list[i]:i for i in range(0, n)}
    index2letter = {i:letter_list[i] for i in range(0, n)}
    return letter2index, index2letter

letter2index, index2letter = create_dictionaries(LETTER_LIST)

'''
Loading all the numpy files containing the utterance information and text information
- Input: training utterances each of variable duration and 40 frequency bands (L, F=40)
- Target: the transcripts corresponding to the utterances in train/dev; Shape: (B, )
'''
def load_data(dataPath):
    speech_train = np.load(dataPath+"train.npy", allow_pickle=True, encoding='bytes')
    speech_dev = np.load(dataPath+"dev.npy", allow_pickle=True, encoding='bytes')
    speech_test = np.load(dataPath+"test.npy", allow_pickle=True, encoding='bytes')

    transcript_train = np.load(dataPath+"train_transcripts.npy", allow_pickle=True, encoding='bytes')
    transcript_dev = np.load(dataPath+"dev_transcripts.npy", allow_pickle=True, encoding='bytes')

    return speech_train, speech_dev, speech_test, transcript_train, transcript_dev

'''
Transforms alphabetical input to numerical input, replace each letter by its corresponding
index from letter_list
  '<sos>': 33
  '<eos>': 34
'''
def transform_letter_to_index(transcript):
    '''
    :param transcript :(N, ) Transcripts are the text input
    :- letter2index: Letter list defined above, map letter to index from LETTER_LIST
    :return letter_to_index_list: Returns a list for all the transcript sentence to index
    '''
    letter_to_index_list = []
    for sent in transcript:
        letters = [letter2index['<sos>']] # start of sequence
        for word in sent:
            # Converte from byte format to string for mapping
            s = word.decode('utf-8')
            for c in s:
                letters.append(letter2index[c])
            # Space between each word
            letters.append(letter2index[' '])
        # pop the ending space, and replace with '<eos>'
        letters.pop()
        letters.append(letter2index['<eos>'])
        letter_to_index_list.append(letters)
    return letter_to_index_list


def transform_index_to_letter(index, stopIdxs, igonre_idxs=[]):
    index_to_letter_list = []
    for r in index:
        curr = ""
        for i in r:
            # Reached the end of the sentence
            if i in stopIdxs:
                break
            elif i in igonre_idxs:
                continue
            else:
                curr += index2letter[i]
        index_to_letter_list.append(curr)
    return index_to_letter_list

data_path = 'hw4_data/'
speech_train, speech_dev, speech_test, transcript_train, transcript_dev = load_data(data_path)

## Debug
print(type(speech_train))
print(type(speech_train[0]))
print(speech_train.shape)
print(speech_train[0].shape)
print(speech_train[0])
print(transcript_train.shape)
print(transcript_train[0])

for word in transcript_train[0]:
  # convert from byte format to string
  print('Byte Word: {}, Converted Word: {}'.format(word, word.decode('utf-8')))

# Preprocess transcript to char level index
character_text_train = transform_letter_to_index(transcript_train)
character_text_dev = transform_letter_to_index(transcript_dev)

## Debug
print(transcript_train.shape)
print(transcript_train[0])
print(len(character_text_train))
print(character_text_train[0])

print("Raw script: \n", transcript_dev[0])
print("Indexed script: \n", character_text_dev[0])
print("Recovered script: \n", transform_index_to_letter(character_text_dev, [])[0])
print("Recovered script with no special character(s): \n", transform_index_to_letter(character_text_dev, [letter2index['<eos>'], letter2index['<pad>']], igonre_idxs=[letter2index['<sos>']])[0])

## Load Dataset from Generator
def gen_dev():
  for utter, trans in zip(speech_dev, character_text_dev):
    # print(utter.shape) # comment during use...
    utter.astype(np.float32)
    
    utter_len = len(utter)
    trans_len = len(trans)
    # print(utter, trans, utter_len, trans_len)
    yield utter, trans, utter_len, trans_len

def gen_train():
  for utter, trans in zip(speech_train, character_text_train):
    # print(utter.shape) # comment during use...
    utter.astype(np.float32)
    
    utter_len = len(utter)
    trans_len = len(trans)
    # print(utter, trans, utter_len, trans_len)
    yield utter, trans, utter_len, trans_len

# Create dataset from generator
# The output shape is variable: (None,)
# dataset = tf.data.Dataset.from_generator(gen, tf.int64, tf.TensorShape([None]))

# '''args have to be Tensor...'''
ds = tf.data.Dataset.from_generator(gen_dev, output_types=(tf.float32, tf.int32, tf.int32, tf.int32), output_shapes=(tf.TensorShape([None, 40]), tf.TensorShape([None]), (), ())) #tf.data.Dataset.from_generator(gen, args=[], output_types=tf.float32, output_shapes = (), )

ds_train = tf.data.Dataset.from_generator(gen_train, output_types=(tf.float32, tf.int32, tf.int32, tf.int32), output_shapes=(tf.TensorShape([None, 40]), tf.TensorShape([None]), (), ()))

print(len(list(ds.as_numpy_iterator())))
for batch in ds.take(2):
  print(type(batch))
  print(batch[0].shape)
  print(batch[0].numpy())
  print(batch[1].shape)
  print(batch[1].numpy())
  print(batch[2])
  print(batch[3])
  print("=====================")

"""### PreProcessing"""

## Pre-Processing

### shuffle
ds_shuffle = ds.shuffle(1024)

ds_train_shuffle = ds_train.shuffle(25000)

print(len(list(ds_shuffle.as_numpy_iterator())))
for batch in ds_shuffle.take(2):
  print(type(batch))
  print(batch[0].shape)
  print(batch[0].numpy())
  print(batch[1].shape)
  print(batch[1].numpy())
  print(batch[2])
  print(batch[3])
  print("=====================")

# lib padded batching
# Batching tensors with padding
batched_ds = ds.padded_batch(32)

batched_ds_train = ds_train_shuffle.padded_batch(64) # OOM DEBUG

print(len(list(batched_ds.as_numpy_iterator())))
### Generator Padded inputs
for batch in batched_ds.take(1):
  print(type(batch))
  print(type(batch[0]))
  print(batch[0].shape)
  print(batch[0][0].numpy()) # first utterance (T, C/F=40)
  print(batch[2].shape)
  print(batch[2].numpy())


for batch in batched_ds_train.take(1):
  print(type(batch))
  print(type(batch[0]))
  print(batch[0].shape)
  print(batch[0][0].numpy()) # first utterance (T, C/F=40)
  print(batch[2].shape)
  print(batch[2].numpy())

### Generator Padded targets
for batch in batched_ds.take(1):
  print(batch[3].numpy())
  print(batch[1].numpy())

## map/collate func

def collate_fn(input, target, input_len, target_len):
  # print(input.eval(session=tf.compat.v1.Session()))
  inputs_pad = []
  inputs_len = [] # same as input_len
  targets_pad = []
  targets_len = []
  
  for i in range(len(input)):
    utter_len = input_len[i]
    real_utter = input[i][:utter_len]
    inputs_pad.append(real_utter)

    trans_len = target_len[i]
    real_trans = target[i][1:trans_len] # ignore '<sos>'
    targets_pad.append(real_trans)
    targets_len.append(len(real_trans)) # tarns_len - 1
  
  # (B, T, F/C)
  padded_seqs = tf.keras.preprocessing.sequence.pad_sequences(inputs_pad, padding='post', dtype='float32')
  padded_targets = tf.keras.preprocessing.sequence.pad_sequences(targets_pad, padding='post', dtype='int32')
  # print(type(padded_seqs), type(padded_targets), type(input_len), type(targets_len))
  return tf.convert_to_tensor(padded_seqs), tf.convert_to_tensor(padded_targets), input_len, tf.convert_to_tensor(targets_len)

result = batched_ds.map(lambda input, target, input_len, target_len : tf.py_function(func=collate_fn, inp=[input, target, input_len, target_len], Tout=[tf.float32, tf.int32, tf.int32, tf.int32]))   # batched_ds.map(collate_fn)

result_train = batched_ds_train.map(lambda input, target, input_len, target_len : tf.py_function(func=collate_fn, inp=[input, target, input_len, target_len], Tout=[tf.float32, tf.int32, tf.int32, tf.int32]), num_parallel_calls=tf.data.AUTOTUNE,
    deterministic=False)
print(len(list(result.as_numpy_iterator())))

print(len(list(result_train.as_numpy_iterator())))

## DEBUG
### Compare first utterance
for d, b1, b2 in zip(speech_dev[:32], batched_ds.take(1), result.take(1)): #First batch
  print(b1[0].shape)
  print(b2[0].shape)
  print(b1[0][0].numpy())
  print(b2[0][0].numpy())
  diff = b1[0][0].numpy() == b2[0][0].numpy()
  print(diff)
  print(len(np.where(diff == False)[0]))

  # original
  print(d)
  utter = b2[0][0].numpy()
  # print(utter[:d.shape[0]])
  print(utter[:b2[2].numpy()[1]])
  diff_ = d == utter[:b2[2].numpy()[0]] # get utterence_len for first utterence
  print(diff_)
  print(len(np.where(diff_ == False)[0]))

"""### Model Structure

### Utilities, Sanity Check
"""

# Helper, Debug func
def inspect_wieghts(model):
  ws = model.get_weights()
  train_vars = model.trainable_variables
  print(type(ws))
  print(len(ws))

  num_vars = 0

  for w, var in zip(ws, train_vars):
    print("Variable: "+var.name)
    print(w.shape)
    print(w)
    cur_num_vars = 1
    for i in range(len(w.shape)):
      cur_num_vars = cur_num_vars * w.shape[i]
    num_vars  = num_vars + cur_num_vars
  print("Total Trainable variables: ", num_vars)



"""### Encoder"""

# One Example batch (B=32)
example_batch = next(iter(result))
print(example_batch[0].shape)
print(example_batch[1].shape) # (B, max taraget length)

"""#### Pyramidal BiLSTM"""

'''
def encoder_fn(lstm_dim, input_dim=40):
    OUTPUT_CHANNELS = 3
    # B x L(max_len of current batch)x C
    inputs = L.Input(shape=(None, input_dim), name='input utterances')

    ### 
    # return the final state
    x, final_memory_state, final_carry_state = L.LSTM(lstm_dim, return_sequences=True, return_state=True, name='enc_dummy_lstm')(inputs) # B x L x H

    print("output Shape ", x.shape)
    print("Memory State Shape ", final_memory_state.shape)
    print("Carry State Shape ", final_carry_state.shape)
    ### Bi-LSTM
    # lstm_layer = L.LSTM(lstm_dim, return_sequences=True, return_state=False) # return the final state (tuple)
    # x = L.Bidirectional(lstm_layer)(inputs) # B x L x 2*H

    

    model = Model(inputs, [x, final_memory_state, final_carry_state])
    
    return model #, (final_memory_state, final_carry_state)
'''

'''
  @param vocab_size: dictionary size
  @states each state should [B, dec_hidden]
'''


class pBLSTM(tf.keras.Model):
  def __init__(self, hidden_dim):
    super(pBLSTM, self).__init__()
    
    self.hidden_dim = hidden_dim
    ##________ Bi-LSTM layer ------- ##
    self.lstm = L.LSTM(self.hidden_dim, return_sequences=True, return_state=True, name='pBLSTM_lstm')
    # outputs of the forward and backward RNNs will be combined using default: concatenate
    self.blstm = L.Bidirectional(self.lstm) # 2*hidden_dim

  # @tf.function
  def call(self, x_padded):
    '''
    param x_padded: padded input, (B, T_max, F)
    param x_lens: actual senquence length (B, ) TODO
    '''

    # Debug
    # print("Input Batch Shape: ", x_padded.shape)
    # print("Runtime sequence shape: ", tf.shape(x_padded)[1])
    '''
      Tensor.shape: The returned tf.TensorShape is determined at build time, without executing the underlying kernel. It is not a tf.Tensor. 
      If you need a shape tensor, either convert the tf.TensorShape to a tf.constant, 
      or use the tf.shape(tensor) function, which returns the tensor's shape at execution time.
    '''
    # chop off extra odd timestamp
    # x_padded = x_padded[:, :(x_padded.shape[1] // 2) * 2, :] # (B, T_max*, F)
    x_padded = x_padded[:, :(tf.shape(x_padded)[1] // 2) * 2, :] # (B, T_max*, F)
    # print("Chop off :", x_padded.shape)

    # reshape to (B, T_max*/2, 2*F)
    ## Output shape of 'Reshape': (batch_size,) + target_shape
    x_paird = L.Reshape(target_shape=(x_padded.shape[1] // 2, x_padded.shape[2] * 2))(x_padded)
    # x_paird = L.Reshape(target_shape=(tf.shape(x_padded)[1] // 2, tf.shape(x_padded)[2] * 2))(x_padded)
    ### Temporrary solution using shape inference
    x_paird = L.Reshape(target_shape=(-1, x_padded.shape[2] * 2))(x_padded)

    # print("REshaped: ", x_paird.shape)

    # Output of Nets, [Forward Net States pair], [Backward Net States pair]
    ### Notice hb, cb corresponding to T=0 in Forward Net $$$
    output, hf, cf, hb, cb = self.blstm(x_paird) # (B, T_max*/2, 2*H)
    
    # return the final output ( forward and backward RNNs) for now
    return output, hf, cf, hb, cb

blstm = pBLSTM(hidden_dim=2)
# blstm.summary()

## Debug length chop off
ex_input = tf.fill([32, 101, 40], 1)
ex_input = tf.cast(ex_input, dtype=tf.float32)
print(ex_input.shape, ex_input.dtype)

ex_out = blstm(ex_input)

print(len(ex_out))
print(ex_out[0].shape)

# DEBUG
example_pblstm_output = blstm(example_batch[0])

print(example_pblstm_output[0].shape)

blstm.summary()
inspect_wieghts(blstm)

"""### Attention"""

## Dev
# a = tf.fill([2, 3, 3], 2)
a = tf.constant([[1, 2, 1], [3,2,4], [2, 1, 1]])
a = tf.reshape(tf.stack([a, a], axis=0), [2, 3, 3])
print(a)
b = tf.constant([[1, 2, 1], [3, 4,2 ]])
print(b)
b = tf.expand_dims(b, axis=2)
print(b)

c = tf.squeeze(tf.matmul(a, b))

print(c)

d = tf.nn.softmax(tf.cast(c, tf.float32))
print(d)

print(tf.reduce_sum(d, axis=1))

'''
 TODO: 
'''
class Attention(L.Layer):
  def __init__(self):
    super().__init__()
    
    # TODO Keras Attention
    # self.attention = tf.keras.layers.AdditiveAttention()

  def call(self, query, value):
    '''
    :param query: (B, dec_hidden) decoder hidden state for current timestamp, s_i
    :param value: (B, T_max, enc_size) encoder high level feature representation, 
                  key && value projections for now (TODO), h
    : TODO, currently dec_hidden == enc_size for simple scalar dot
    '''
    # print("Query Size: ", query.shape)
    # print("Value size: ", value.shape)
    # compute the energy
    # (B, T_max), scalar enery ei,u for each input timestep u, under current decoder step i
    ## pass in 'axis=' to tf.squeeze, remove only the current transcript ts dimension, which is 1. 
    ## - this is to prevent squeezing batch dimension when B==1
    energy = tf.squeeze(tf.matmul(value, tf.expand_dims(query, axis=2)), axis=2)

    # print("Energy (B, T_max): ", energy.shape)

    # TODO: masking the padded ts
    # Attention vector, softmax, default axis=-1
    # (B, T_max)
    attention = tf.nn.softmax(energy)

    # Context vectir c_i
    # value same as key bere
    # (B, value_size)
    context = tf.squeeze(tf.matmul(tf.expand_dims(attention, axis=1), value), axis=1)

    return context, attention

attention = Attention()

## Debug
ex_query = tf.fill([32, 4], 1.0)
ex_value = tf.fill([32, 5, 4], 3.0)
att_out, att = attention(ex_query, ex_value)

print(att_out.shape)
print(att.shape)
print(tf.reduce_sum(att, axis=1))

# Dev
a = tf.constant([1, 2, 3, 4, 5, 6], shape=[1, 2, 3])
# print(a)
b = tf.constant([7, 8, 9, 10, 11, 12], shape=[1, 3, 2])

tf.matmul(a, b)

print("raw query: ", ex_query.shape)
e_query = tf.expand_dims(ex_query, axis=2)

print(e_query)

tf.matmul(ex_value, e_query)

"""### Decoder"""

example_output, example_h, example_c = encoder(example_batch[0])
print(type(example_output))
print ('Encoder output shape: (batch size, sequence length, units/enc_hidden) {}'.format(example_output.shape))
print ('Encoder h vecotr shape: (batch size, units) {}'.format(example_h.shape))
print ('Encoder c vector shape: (batch size, units) {}'.format(example_c.shape))

print('hidden state of last timestep')
print(example_h.numpy())
print('Last Feature output of LSTM')
print(example_output.numpy()[:, -1, :])

example_ts_target = example_batch[1][:, 0]
print(example_batch[1].numpy())
print(example_ts_target.numpy())

"""#### Module Decoder"""

## Debug
example_batch[1].shape

'''
  @param vocab_size: dictionary size
  @param dec_hidden: each state should [B, dec_hidden], TODO: same as encoder value projection size for now
'''


class Decoder(tf.keras.Model):
  def __init__(self, vocab_size, embed_dim, dec_hidden):
    super(Decoder, self).__init__()
    
    self.dec_units = dec_hidden
    self.embedding = L.Embedding(vocab_size, embed_dim)

    ##________ Step LSTM Cell in Decoder ------- ##
    self.lstm1 = L.LSTMCell(dec_hidden, name='dummy_lstm_cell')

    self.attention = Attention()

    # char distribution
    self.fc = L.Dense(vocab_size)


  # @tf.function
  def call(self, x, value=None, hidden=None):
    '''
    :param x: (B, L_max) [ground-trouth, D] input transcrip sequence
    :param value: (B, T_max, enc_size); high-level feature representation of encoder projection, h
    :TODO, separately projected key of encoder
    :param hidden: s-1, tuple of (hidden_state, cell_state)
    '''

    # print("Input Transcripts Shape: ", x.shape)
    batch_size = x.shape[0]
    L = x.shape[1] # max transcript length for current 
    
    
    hidden_states = hidden

    # (B, L_max, E) 
    # the '<sos>' embedding is not included
    x = self.embedding(x)
    # print("Embedded input Shape: ", x.shape)

    logits = []

    for i in range(L):
      # for each timestamp
      ## 1. Get Input Transcript embedding (B, E)
      # if i == 0:
      #   char_embed = tf.zeros([batch_size], tf.float32)
      # else:
      #   char_embed = x[:, i-1, :] 
      
      # <sos> included, <eos> not included
      # (B, E), for current ts
      char_embed = x[:, i, :]

      # print("TS: {}, embedding: {}".format(i, char_embed.shape))

      ## 2. pass through rnn cell
      ### out/query (B, dec_hidden)
      out, states = self.lstm1(inputs=char_embed, states=hidden_states)
      hidden_states = states

      # print("LSTM out", out.shape)
      # print(out.numpy())
      # print(type(states))
      # print(len(states))
      # print(states[0].numpy())
      # print(states[1].numpy())

      ## 3. get context vector using query and key
      ### context: (B, value_size/enc_size/dec_size)
      # print("Compute attention for ts: {}".format(i))
      # print("Enc value projection: ", value.shape)
      context, attention = self.attention(out, value)

      # print(context.shape)
      # print(context)
      # print(tf.reduce_sum(attention, axis=1))

      ## 4. get character distribution (Final Linear layer)
      ### concatenate rnn output and context vector: (B, dec_size|value_size)
      cat = tf.keras.layers.concatenate([out, context], axis=-1)
      # print(cat.shape)

      # (B, vocab_size)
      char_dis = self.fc(cat)
      # print(char_dis.shape)
      # print(tf.reduce_sum(char_dis, axis=1))

      # print(char_dis.shape)
      # (B, 1, vocab_size)
      logits.append(tf.expand_dims(char_dis, axis=1))
      
    # (B, L_max, vocab_size)
    output = tf.concat(logits, axis=1)
    # print(output.shape)
    # output, h, c = self.lstm_layer(x, initial_state = hidden)
    # dis = self.fc(output)
    # return dis, h, c

    # return lstm next hidden state && cell state for each element in the batch: (B, dec_H)
    return output, hidden_states

# take in variable length input
decoder_ = Decoder(len(LETTER_LIST), 100, 128)
# decoder_.build(tf.TensorShape(None))
# inspect_wieghts(decoder_)


# example_dec_output, ex_dec_h, ex_dec_c = decoder_(example_batch[1], value=tf.ones([32, 201, 128]),hidden=[tf.zeros([32, 128]), tf.zeros([32, 128])])
# input tuple of (hidden_state, cell_state)
example_dec_output, example_dec_next_states = decoder_(example_batch[1], value=tf.ones([32, 201, 128]),hidden=[tf.zeros([32, 128]), tf.zeros([32, 128])])
# decoder_.summary()
# inspect_wieghts(decoder_)
print(example_dec_output.shape)
print(example_dec_next_states[0].shape)
print(example_dec_next_states[1].shape)

print(example_dec_output.shape) # （B, T_max, V)
print ('Decoder h vecotr shape: (batch size, dec_units) {}'.format(ex_dec_h.shape))
print ('Decoder c vector shape: (batch size, dec_units) {}'.format(ex_dec_c.shape))

"""#### E2E test"""

print("Input Utterence: ", example_batch[0].shape)
print("Target Transcripts: ", example_batch[1].shape)
encoder = pBLSTM(hidden_dim=64)

example_enc_output = encoder(example_batch[0])

print(example_enc_output[0].shape)
print(example_enc_output[1].shape)
print(example_enc_output[2].shape)

dec_ini_h = tf.concat([example_enc_output[1], example_enc_output[3]], axis=1)
dec_ini_c = tf.concat([example_enc_output[2], example_enc_output[4]], axis=1)
print(dec_ini_h.shape)
print(dec_ini_c.shape)
# print(example_enc_output[0].numpy()[:, -1, :].shape)
# print(example_enc_output[1].numpy())

decoder = Decoder(len(LETTER_LIST), 100, 128)

# input 1: target transcript (B, L)
# input 2: encoder value proj (B, T, E_H), encoder feature representation
# input 3: initial hidden states for encoder LSTM(s)
ex_dec_output, ex_dec_next_states = decoder(example_batch[1], value=example_enc_output[0],hidden=[dec_ini_h, dec_ini_c])

print(ex_dec_output.shape)
print(ex_dec_output.numpy()[0])
print(ex_dec_next_states[0].shape)
print(ex_dec_next_states[1].shape)

encoder.summary()
decoder.summary()
inspect_wieghts(decoder)

l_t = [1, 2, 3]
l_t[:-1]

"""## Train"""

def plot_grad_flow(grads_and_vars):
  '''
  input (gradients, trainable_variables)
  '''
  grads = grads_and_vars[0]
  vars = grads_and_vars[1]

  ave_grads = []    
  max_grads= []    
  layers = [] 

  print("# variables {}, # grads {}".format(len(vars), len(grads)))

  for grad, var in zip(grads, vars):
    name = var.name
    layers.append(name)

    # reduce over all dims/elements                
    ave_grads.append(tf.reduce_mean(tf.abs(grad)))     
    max_grads.append(tf.reduce_max(tf.abs(grad)))

  plt.clf()
  plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color="r")    
  plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color="b")    
  plt.hlines(0, 0, len(ave_grads)+1, lw=2, color="k" )    
  plt.xticks(range(0,len(ave_grads), 1), layers, rotation="vertical")    
  plt.xlim(left=0, right=len(ave_grads))    
  # plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions    
  plt.xlabel("Layers")
  plt.ylabel("average gradient")    
  plt.title("Gradient flow")    
  #plt.tight_layout()    
  plt.grid(True)    
  # plt.legend([Line2D([0], [0], color="c", lw=4),
  #             Line2D([0], [0], color="b", lw=4),
  #             Line2D([0], [0], color="k", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])    
  return plt

def loss_function(real, pred, target_mask):
  # loss always reduce -1 dimension
  # real shape = (BATCH_SIZE, max_length_output)
  # pred shape = (BATCH_SIZE, max_length_output, target_vocab_size )
  # print("Target Shape ", real.shape)
  # print("Logits Shape ",pred.shape)
  # print("Target Lens ", target_lens.shape)
  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')
  # (BATCH_SIZE, max_length_output)
  loss = cross_entropy(y_true=real, y_pred=pred)
  # print("Loss Matrix Shape ", loss.shape)
  ## TODO: Mased Loss
  # mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1
  # mask = tf.cast(mask, dtype=loss.dtype)  
  # loss = mask* loss

  # # Mask based on target lens
  # # TODO: 'None values not supported.'
  # max_len = real.shape[1]
  # b_size = real.shape[0]

  # # B x L_Max
  # lens_m = tf.repeat(tf.expand_dims(tf.range(0, max_len), axis=0), repeats=b_size, axis=0)
  # # print(lens_m)

  # # B x L_Max
  # lens_e = tf.repeat(tf.expand_dims(target_lens, axis=1), repeats=max_len, axis=1)
  # # print(lens_e)

  # mask = lens_m < lens_e
  # # print(mask)

  # mask = tf.cast(mask, dtype=loss.dtype)
  # # print(mask)

  # # print('Mask sum at transcript i: {}, actual transcript length: {}'.format(tf.reduce_sum(mask[5]), target_lens[5]))
  masked_loss = target_mask * loss
  final_loss = tf.reduce_mean(masked_loss)
  return final_loss, tf.reduce_mean(loss)

optimizer = tf.keras.optimizers.Adam()

### TODO: under tf.function, get runtime sequence shape (errors under 'None' runtime value, both L_max and T_max)
# @tf.function( experimental_relax_shapes=True)
def train_step(input, target, target_mask):
  # print("=========== Inspecting weights of encoder")

  # inspect_wieghts(encoder)

  # print("=========== Inspecting weights of decoder")

  # inspect_wieghts(decoder_)

  loss = 0

  # print(input.shape)
  # print(target.shape)
  # print(target.numpy())


  with tf.GradientTape() as tape:
    # enc_output, enc_h, enc_c =  encoder(input)

    enc_output = encoder(input)

    dec_ini_h = tf.concat([enc_output[1], enc_output[3]], axis=1)
    dec_ini_c = tf.concat([enc_output[2], enc_output[4]], axis=1)

    # attach previous removed '<sos>' #TODO, remove this
    # Ignore '<eos>'
    # But still keep the '<eos>' token in target, - we still need to learn when to end the sequence 
    dec_input = tf.pad(target[:, :-1], [[0, 0], [1, 0]], "CONSTANT", constant_values=letter2index['<sos>'])
    # real = target[ : , 1: ]         # ignore <start> token
    # target already have '<sos>' removed

    ## $$ use decoder final state as initial states for decoder LSTM cell here
    ## s and h sharing same size, so s-1 = hN
    ## Use encoder direct LSTM output features as 'value' , enc_H == dec_H for sclar dot during attention computation
    ### Connecting Encoder and Decoder here to avoid `WARNING:tensorflow:Gradients do not exist for Encoder Variables when minimizing the loss.`

    # pred, dec_h, dec_c = decoder(dec_input, [enc_h, enc_c]) # input targets
    pred, dec_next_states = decoder(dec_input, value=enc_output[0], hidden=[dec_ini_h, dec_ini_c])
    
    loss, unmasked_loss = loss_function(target, pred, target_mask)

  variables = encoder.trainable_variables + decoder.trainable_variables

  # print("BackWarding: Back Prop loss.......")
  # # Calculate the gradients for encoder and decoder
  # print("Type of loss: {}, value of loss: {}".format(type(loss), loss.numpy()))

  ## returned gradieents are a list or nested structure of Tensors, one for each element in sources. same structure as sources.
  gradients = tape.gradient(loss, variables)
  optimizer.apply_gradients(zip(gradients, variables))

  return loss, unmasked_loss, (gradients, variables)

EPOCHS = 10
data = result_train # dev dataset: result
for epoch in range(EPOCHS):
  start = time.time()
  total_loss = 0
  # print(enc_hidden[0].shape, enc_hidden[1].shape)
  steps_per_epoch = 0

  for (batch, (inp, targ, input_lens, target_lens)) in enumerate(data):
    # print('Batch # {}'.format(batch))
    # Debug
    # print(type(inp), type(targ), type(input_lens), type(target_lens))
    # print(inp.shape)
    # print(targ.shape)
    # print(target_lens)
    # Generate mask before hand
    # Mask based on target lens
    # TODO: put into generate loss function
    max_len = targ.shape[1]
    b_size = targ.shape[0]

    # B x L_Max
    lens_m = tf.repeat(tf.expand_dims(tf.range(0, max_len), axis=0), repeats=b_size, axis=0)
    # print(lens_m)

    # B x L_Max
    lens_e = tf.repeat(tf.expand_dims(target_lens, axis=1), repeats=max_len, axis=1)
    # print(lens_e)

    mask = lens_m < lens_e
    # print(mask)
    mask = tf.cast(mask, dtype=tf.float32)

    # print('Mask sum at transcript i: {}, actual transcript length: {}'.format(tf.reduce_sum(mask[5]), target_lens[5]))

    # Train Current Step
    # print("----Batch {}: train_step".format(batch))
    batch_loss, batch_unmasked_loss, grads_vars = train_step(inp, targ, mask)
    total_loss += batch_loss

    if batch % 10 == 0:
      print('Epoch {} Batch {} Loss {:.4f}; [Debug] Unmasked Loss: {:.4f}'.format(epoch + 1,
                                                   batch,
                                                   batch_loss, batch_unmasked_loss))
                                                  #  batch_loss.numpy())) # tf function
      # [Sanity Check x] Plot Gradient Flow
      if batch % 50 == 0:
        plt = plot_grad_flow(grads_vars)

        plt.show()

    steps_per_epoch += 1
  # # saving (checkpoint) the model every 2 epochs
  # if (epoch + 1) % 2 == 0:
  #   checkpoint.save(file_prefix = checkpoint_prefix)

  print('Epoch {} Loss {:.4f}, after {} steps'.format(epoch + 1,
                                      total_loss / steps_per_epoch, steps_per_epoch))
  print('Time taken for 1 epoch {} sec\n'.format(time.time() - start)) # CPU: 86s for dev_data

  # Resource Monitoring
  monitor_gpu()

## Debug
# for batch in result_train:
#   print(batch[0].shape, batch[1].shape, batch[3].shape)

monitor_gpu()

# debug

inspect_wieghts(decoder)

inspect_wieghts(encoder)

## DEV
t = tf.constant([[1,2,3], [2,3,4], [5,6,6]])
# t = t + tf.fill(3, 33)
f = tf.fill(3, 1)
print(f)

# tf.stack([t, f], axis=0)
# tf.concat([t, f], axis=1)
tf.pad(t, [[0, 0], [1, 0]], "CONSTANT", constant_values=33)

r = tf.random.uniform(shape=[2, 3, 2])
r.numpy()

r_ = r[:, 0, :] # take the first sequence
r_.numpy()



"""## Inference/Test"""

print(example_dec_output.shape) # （B, T_max, vocab_size)
# print(example_dec_output)

test_batch = next(iter(result))
print(test_batch[0].shape)
print(test_batch[1].shape) # (B, max taraget length)

# it = iter(result)
# for b in result.take(10):
#   print(b[0].shape)

def val(num_samples=2, max_seq=20):
  # For single Batch
  utter_input = test_batch[0] # B, U_max, F
  target_trans = test_batch[1] # B, T_max
  batch_size = target_trans.shape[0]
  print(utter_input.shape)
  print(target_trans.shape)

  og_trans = transform_index_to_letter(target_trans.numpy(), [letter2index['<eos>'], letter2index['<pad>']])
  for i in range(target_trans[:min(num_samples, batch_size)].shape[0]):
    utter = utter_input[i:i+1, :, :] # avoid squeezing
    print("===================================Raw Utterance {}: ===================================\n {}".format(i, utter.numpy()))
    target = target_trans[i:i+1, :]
    print(utter.shape) # (1, U, F)
    print(target.shape) # (1, T)
    
    # Invoke Encoder
    # enc_output, enc_h, enc_c =  encoder(utter)

    enc_output = encoder(utter)

    enc_h = tf.concat([enc_output[1], enc_output[3]], axis=1)
    enc_c = tf.concat([enc_output[2], enc_output[4]], axis=1)

    print(type(enc_output))
    print(enc_output[0].shape) # 1, U, enc_dim
    print(enc_h.shape) # 1, enc_dim
    print("Last T of enc out \n", enc_output[0].numpy()[:, -1, :])
    print("Enc Last Hidden state \n", enc_h.shape, enc_h.numpy())
    print("Enc Last Cell state \n", enc_c.shape, enc_c.numpy())


    # attach previous removed '<sos>'
    # Ignore '<eos>', $$$ TODO: need to remove padding
    # dec_input = tf.pad(target[:, :-1], [[0, 0], [1, 0]], "CONSTANT", constant_values=letter2index['<sos>'])
    # print(dec_input)
    dec_input = tf.fill([1, 1], letter2index['<sos>']) # start of sequence # (1, 1)
    seq = ['<sos>']
    seq_index = [letter2index['<sos>']]
    # initial Decoder State
    # Use Encoder hidden states for now, s-1=hN
    h_state = enc_h
    c_state = enc_c

    while tf.squeeze(dec_input).numpy() != letter2index['<eos>'] and len(seq) < max_seq:
      # print(h_state.numpy())
      # print(c_state.numpy())
      # pred, dec_h, dec_c = decoder_(dec_input, [h_state, c_state]) # input targets 
      ## TODO
      pred, next_states = decoder(dec_input, value=enc_output[0], hidden=[h_state, c_state])

      h_state = next_states[0]
      c_state = next_states[1]
      # print("Last Linear Layer :", pred.shape) # (1, 1, 35)
      # print("Hidden state of last timestamp/current :", dec_h.shape) # 1, dec_dim
      # print("Hidden Cell state of last timestamp/current: ", dec_c.shape) # 1, dec_dim
      # print(pred[-1]) # (1, 1, 35)


      # $$ Greedy Search
      next_input =  tf.math.argmax(tf.squeeze(pred), axis=-1)
      dec_input = tf.fill([1,1], next_input)
      seq.append(index2letter[next_input.numpy()])
      seq_index.append(next_input.numpy())

      # print(tf.squeeze(pred).numpy())
      # print(seq)



    # print(utter.numpy())
    print("Input Target: ", target.numpy())
    print("Actual Transcript: ", og_trans[i])

    print("Raw Prediction sequence :", seq)
    print("Predicted Transcript: ", transform_index_to_letter([seq_index], [letter2index['<eos>'], letter2index['<pad>']]))

val(num_samples=10)

